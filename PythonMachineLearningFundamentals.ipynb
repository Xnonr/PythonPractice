{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python PEP 8 styling guide recommends using lower case with underscores as opposed to camel case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EBA)\n",
    "# Why:\n",
    "# Understand the shape of the data\n",
    "# Learn which features might be useful\n",
    "# Inform the cleaning that will come next\n",
    "# How:\n",
    "# Get counts and distributions of all variables, determine each features data type and correlations\n",
    "# and deal with missing data and duplications\n",
    "\n",
    "# Data Cleaning\n",
    "# Why:\n",
    "# Shape data in a fashion a model can best pick up on any availble signals\n",
    "# Remove irrelevant data\n",
    "# Adjust features to be acceptable to the model\n",
    "# How:\n",
    "# Ananonymize and fill in any missing data if necessary, encode categorical variables and prune or scale\n",
    "# the data to account for skewed data and or outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Sklearn will send a warning when it's methods will change in future updates, \n",
    "# this is simply hidding those messages\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "titanic_df = pd.read_csv('../../Data/TitanicTrainingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f648c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30af3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all categorical features\n",
    "# Mainly for ease of data exploration at the moment, some of these features are potentially useful\n",
    "categorical_features = ['PassengerId', 'Name', 'Ticket', 'Sex', 'Cabin', 'Embarked']\n",
    "titanic_df.drop(categorical_features, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring continous features\n",
    "# Age has missing data values base on the available count\n",
    "# Survived is a binary feature whose mean can be used to determine the percentage of surviving passengers, ~38%\n",
    "titanic_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba95144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for which features may be significant in terms of determining a passenger's survival\n",
    "# Fare and passenger class appear at first glance to be possible leads\n",
    "titanic_df.groupby('Survived').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7190ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining whether or not the missing age values are random or in a systematic way\n",
    "# For example, the ages of those in first class might not have been asked their ages\n",
    "# This determines how the missing data should be handled\n",
    "# Those whose ages weren't recorded seemed to have paid lower fares, being in lower classes, as well as having\n",
    "# traveled with fewer people on average, which means those in the bowels of the ship might have avoided records\n",
    "# Otherwise, nothing in particular stands out signifying the missing data has to be treated in a specific manner\n",
    "titanic_df.groupby(titanic_df['Age'].isnull()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting continous features\n",
    "# Desire to understand the shape of the features and how they relate to the target variable\n",
    "# Overlaid histograms between survivors and those who perished is one way to accomplish this\n",
    "# Utilizing the continous variables of 'Age' and 'Fare'\n",
    "for i in ['Age', 'Fare']:\n",
    "    # Grabbing non missing values into 2 lists of the dead and survivors\n",
    "    died = list(titanic_df[titanic_df['Survived'] == 0][i].dropna())\n",
    "    survived = list(titanic_df[titanic_df['Survived'] == 1][i].dropna())\n",
    "    \n",
    "    x_min = min(min(died), min(survived))\n",
    "    x_max = max(max(died), max(survived))\n",
    "    width = (x_max - x_min) / 40\n",
    "    \n",
    "    # Plotting the 2 lists of grabbed values onto an overlaid histogram\n",
    "    sns.displot(died, color = 'r', kde = False, bins = np.arange(x_min, x_max, width))\n",
    "    sns.displot(survived, color = 'g', kde = False, bins = np.arange(x_min, x_max, width))\n",
    "    \n",
    "    plt.legend(['Perished', 'Survived'])\n",
    "    plt.title('Overlaid Histogram For {}'.format(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177cdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical plot to determine survival rate based upon the categorical features various levels\n",
    "# Telling seaborn to create a new plot for every iteration through the loop\n",
    "# Presents the percent of people whom survived based on the categorical variables values, along with error bars\n",
    "# The larger the ammount of data, the smaller the error bar and the greater confidence\n",
    "# Passenger class appears to have an obvious impact on survival, as well as those with many familly members\n",
    "for index, column in enumerate(['Pclass', 'SibSp', 'Parch']):\n",
    "    plt.figure(index)\n",
    "    sns.catplot(x = column, y = 'Survived', data = titanic_df, kind = 'point', aspect = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69dab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the parents and siblinbs columns into a single value column\n",
    "titanic_df['FamilyCount'] = titanic_df['SibSp'] + titanic_df['Parch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined columns' patterns generally persist, whith those with many familly members being more likely\n",
    "# to perish in the sinking\n",
    "# If you can combine columns whilst maintaining the same general pattern or trend it is a good idea to do so\n",
    "# It cleans up the data by given the model fewer things to look at\n",
    "sns.catplot(x = 'FamilyCount', y = 'Survived', data = titanic_df, kind = 'point', aspect = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1caaf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continous data cleaning\n",
    "# Dropping irrelevant continous variables\n",
    "# 'inplace': Whether to create a new standalone dataframe or alter the current dataframe\n",
    "titanic_df.drop('PassengerId', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e12641",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e280e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling in the missing age variable values\n",
    "# Using the most naive method to do this by filling in the missing values with the average age,\n",
    "# avoiding biasing the model towards one direction or another\n",
    "titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that there are no longer any missing values within the dataframe's columns\n",
    "titanic_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9eaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping individual columns used to create the combined column to avoid multicollinearity issues because\n",
    "# they are highly correlated and will otherwise confuse the model\n",
    "titanic_df['FamilyCount'] = titanic_df['SibSp'] + titanic_df['Parch']\n",
    "titanic_df.drop(['SibSp', 'Parch'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d892b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring categorical features\n",
    "# Often done seperately due to the different methods in exploring continous and categorical features\n",
    "continous_features = ['PassengerId', 'Pclass', 'Name', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "titanic_df.drop(continous_features, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a general overhead view\n",
    "# Quite a few missing values for the 'Cabin' feature\n",
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dramatic split between those whose cabin numbers were recorded and not missing (False) and those's who were \n",
    "# (True) in terms of their average chances of survival\n",
    "# The missing value must mean something as so should be handled in a specific fashion\n",
    "titanic_df.groupby(titanic_df['Cabin'].isnull()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the passenger had a cabin, then 1, if not then 0\n",
    "# Encoded as indicator variable since the cabin feature means something\n",
    "titanic_df['CabinIndicator'] = np.where(titanic_df['Cabin'].isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1172cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b58db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting categorical variables\n",
    "for index, column in enumerate(['CabinIndicator', 'Sex', 'Embarked']):\n",
    "    plt.figure(index)\n",
    "    sns.catplot(x = column, y = 'Survived', data = titanic_df, kind = 'point', aspect = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafcd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the survival rating based on port of embarkation by comparing to 'Sex'\n",
    "# Fewer people survived from Southhampton because they were mostly men\n",
    "titanic_df.pivot_table('Survived', index = 'Sex', columns = 'Embarked', aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82950f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the survival rating based on port of embarkation by comparing to whether or not the had a cabin\n",
    "# More people from Cherbourg survived as they had cabins\n",
    "# Overall embarked is repetetive in an indirect fashion by showing the very strong features of sex and cabin\n",
    "# ownership and is in it of itself not that useful to the model, and so should be done away with\n",
    "titanic_df.pivot_table('Survived', index = 'CabinIndicator', columns = 'Embarked', aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the categorical data\n",
    "titanic_df.drop(['Name', 'Ticket'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b157958",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'Sex' variable feature to numeric\n",
    "# Dictionary to handle the gender to numeric mapping which will then be \n",
    "# applied to the Titanic dataframe's 'Sex' column\n",
    "gender_number = {'male': 0, 'female': 1}\n",
    "\n",
    "titanic_df['Sex'] = titanic_df['Sex'].map(gender_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb129ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdefa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'Cabin' and 'Embarked' feature columns\n",
    "titanic_df.drop(['Cabin', 'Embarked'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f49aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bff421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "# Training: Used to train the model and where the algorithm is allowed to learn from\n",
    "# Validation: Used to select the best model for the most optimal algorithm and hyperparameter tuning\n",
    "# Testing: Used to provide and unbiased evaluation of what the model will look like in a real environment\n",
    "\n",
    "features = titanic_df.drop('Survived', axis = 1)\n",
    "labels = titanic_df['Survived']\n",
    "\n",
    "# Initializion seed in order to reproduce the same output when the code is run\n",
    "# The 'train_test_split' cannot split a dataset into 3 groups, so to obtain a validation dataset a 2 step\n",
    "# process has to be carried out\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.4, random_state = 42)\n",
    "x_test, x_validation, y_test, y_validation = train_test_split(x_test, y_test, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eda939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels), len(y_train), len(y_validation), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout Test Set: Essentially the validation and testing datasets\n",
    "# Sample of data not used in fitting a model, used to evaluate the model's ability to generalize to unseen data\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "# Data is divided into k subsets and the holdout method is repeated k-times. Each time one of the the k-subsets\n",
    "# is used as the test set and the other k-1 subsets are combined to be used to train the model\n",
    "\n",
    "# Evaluation Framework\n",
    "# Metrics: How is the model's accuracy gauged\n",
    "# Process: How to split the data, how is the full dataset leveraged to mitigate the \n",
    "# likelyhood of overfitting & underfitting\n",
    "# 1. Run 5 fold cross validation on the training dataset and select the best models\n",
    "# 2. Re-fit the models on a full training dataset, evaluate those models on \n",
    "# the validation dataset and pick the best on\n",
    "# 3. Evaluate the best model on the test dataset to gauge its ability to generalize to unseen data\n",
    "\n",
    "# Titanic passenger survival is a classification problem\n",
    "# Accuracy = # predicted correctly / total # of examples\n",
    "# Precision = # predicted as surviving that actually survived / total # predicted to survive\n",
    "# Recall = # predicted as surviving that actually survived / total # that actually survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f90786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias: Algorithm's tendency to consistently learn the wrong thing by not taking into account all of the \n",
    "# the information in the data\n",
    "\n",
    "# High Bias: Result of the algorithm missing the relevant relations between features and target outputs\n",
    "\n",
    "# Variance: An algorithm's sensitivity to small fluctuations in the training set\n",
    "# High Variance: Result of the algorithm fitting to random noise in the training data\n",
    "\n",
    "# Bias Variance Tradeoff: Using dart board analogy\n",
    "# Low Bias + Low Variance: All darts in the bullsey\n",
    "# Low Biad + High Variance: Darts spread out around the bullsey\n",
    "# High Bias + Low Variance: All darts condensed somewhere other than the bullsey\n",
    "# High Bias + High Variance: Darts spread out around a point other than the bullsey\n",
    "\n",
    "# Total Error = (Bias + Variance) + Irreducible Noise\n",
    "\n",
    "# Models:\n",
    "# Simple: Underfitting, low complexity and variance, high bias, training and testing error\n",
    "# Good: Medium complexity, low bias, variance, training and testing error\n",
    "# Complex: Overfitting, high complexity, variance and testing error, low bias and training error\n",
    "\n",
    "# Hyperparameter Tuning: Choosing a set of optimal hypeparameters for fitting an algorithm\n",
    "# Parameter: Configuration variable that is internal to the model and whose value can be estimated from the data\n",
    "# Hyperparamter: Configuration that is external to the model, whose value cannot be estimated from the data\n",
    "# and whose value guides how the algorithm learns parameter values from the data (Example: Decision Tree Depth)\n",
    "\n",
    "# Regularization: Technique used to avoid overfitting by discouraging overly complex models in some way\n",
    "# Goal is to allow enough flexibity for the algorithm to learn the underlying patterns in the data but provide\n",
    "# guardrails so it doesn't overfit\n",
    "\n",
    "# Ridge & Lasso Regressiob: Adding a penalty to the loss function to constrain coefficients, \n",
    "# the model has to greatly increase in accuracy before it is allowed to become more complex\n",
    "\n",
    "# Dropout: Some nodes are ignored during training which forces the other nodes to take on more or less \n",
    "# responsibility for the input / output, more for deep leaning and neural networks\n",
    "\n",
    "# Occams' Razor: Whenever possible, choose the simplest answer to a problem\n",
    "# Choose the most simple model possible that will provide accurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ee3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Process\n",
    "# Loading in the original Titanic dataset\n",
    "titanic_df = pd.read_csv('../../Data/TitanicTrainingData.csv')\n",
    "\n",
    "# Cleaning the continous features\n",
    "titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace = True)\n",
    "titanic_df['FamilyCount'] = titanic_df['SibSp'] + titanic_df['Parch']\n",
    "titanic_df.drop(['PassengerId', 'SibSp', 'Parch'], axis = 1, inplace = True)\n",
    "\n",
    "# Cleaning the categorical features\n",
    "titanic_df['CabinIndicator'] = np.where(titanic_df['Cabin'].isnull(), 0, 1)\n",
    "gender_number = {'male': 0, 'female': 1}\n",
    "titanic_df['Sex'] = titanic_df['Sex'].map(gender_number)\n",
    "titanic_df.drop(['Cabin', 'Embarked', 'Name', 'Ticket'], axis = 1, inplace = True)\n",
    "\n",
    "# Splitting the dataset\n",
    "features = titanic_df.drop('Survived', axis = 1)\n",
    "labels = titanic_df['Survived']\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.4, random_state = 42)\n",
    "x_test, x_validation, y_test, y_validation = train_test_split(x_test, y_test, test_size = 0.5, random_state = 42)\n",
    "\n",
    "for dataset in [y_train, y_validation, y_test]:\n",
    "    print(round(len(dataset) / len(labels), 2))\n",
    "\n",
    "# Write out to cleaned data\n",
    "# Do not save the index as if the file is read in at a later time by Pandas, it will see the index as a \n",
    "# feature and cout it as a new column\n",
    "titanic_df.to_csv('../../Data/CleanedTitanicData.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d17062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a basic model using 5 fold cross-validation\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# A warning may appear indicating that sklearn prefers column labels as an array rather than a pandas column vector\n",
    "scores = cross_val_score(rf, features, labels.values.ravel(), cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c234ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier to understand results\n",
    "def print_results(results):\n",
    "    print('Best Parameters: {}\\n'.format(results.best_params_))\n",
    "    \n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    \n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fc5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning hyperparameters\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50 , 100],\n",
    "    'max_depth': [2, 10, 20, None]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(rf, parameters, cv = 5)\n",
    "cv.fit(features, labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce30cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating results on validation set\n",
    "\n",
    "# Refiting the best 3 cross validation models on the remaining validation dataset, \n",
    "# as so far only 80% of the training data has benn used\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
    "rf1.fit(features, labels.values.ravel())\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators = 50, max_depth = 100)\n",
    "rf2.fit(features, labels.values.ravel())\n",
    "\n",
    "rf3 = RandomForestClassifier(n_estimators = 50, max_depth = None)\n",
    "rf3.fit(features, labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad957966",
   "metadata": {},
   "outputs": [],
   "source": [
    "for md1 in [rf1, rf2, rf3]:\n",
    "    y_pred = md1.predict(x_validation)\n",
    "    accuracy = round(accuracy_score(y_validation, y_pred), 3)\n",
    "    precision = round(precision_score(y_validation, y_pred), 3)\n",
    "    recall = round(recall_score(y_validation, y_pred), 3)\n",
    "    print('Max Depth: {} / # Of Est: {} -- A: {} / P: {} / R: {}'.format(md1.max_depth, \n",
    "                                                                         md1.n_estimators, \n",
    "                                                                         accuracy, \n",
    "                                                                         precision,\n",
    "                                                                         recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf2.predict(x_test)\n",
    "accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "precision = round(precision_score(y_test, y_pred), 3)\n",
    "recall = round(recall_score(y_test, y_pred), 3)\n",
    "print('Max Depth: {} / # Of Est: {} -- A: {} / P: {} / R: {}'.format(md1.max_depth,\n",
    "                                                                     md1.n_estimators, \n",
    "                                                                     accuracy,\n",
    "                                                                     precision,\n",
    "                                                                     recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f9364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
