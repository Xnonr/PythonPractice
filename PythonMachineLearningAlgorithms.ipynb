{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alogrithm: Mathematical technique or equation, a framework for a model\n",
    "# Model: Equation that is formed by using data to find the parameters in the equation of an algorithm\n",
    "\n",
    "# Machine Learning Model Steps\n",
    "# 1. Explore and clean the dataset\n",
    "# 2. Split the dataset into training, validation and testing datasets\n",
    "# 3. Fit an initial model and evaluate it\n",
    "# 4. Tune hyperparameters\n",
    "# 5. Evaluate on the validation dataset\n",
    "# 6. Select and evaluate the final model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a94504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)\n",
    "#warnings.filterwarnings('ignore', category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading In The Data\n",
    "titanic = pd.read_csv('../../Data/TitanicTrainingData.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Continous Variables\n",
    "titanic['Age'].fillna(titanic['Age'].mean(), inplace = True)\n",
    "titanic['FamilyCount'] = titanic['SibSp'] + titanic['Parch']\n",
    "titanic.drop(['PassengerId', 'SibSp', 'Parch'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a33b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Categorical Variables\n",
    "titanic['CabinIndicator'] = np.where(titanic['Cabin'].isnull(), 0, 1)\n",
    "gender_number = {'male': 0, 'female': 1}\n",
    "titanic['Sex'] = titanic['Sex'].map(gender_number)\n",
    "titanic.drop(['Cabin', 'Embarked', 'Name', 'Ticket'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting The Dataset\n",
    "# x: Features\n",
    "# y: Labels\n",
    "features = titanic.drop('Survived', axis = 1)\n",
    "labels = titanic['Survived']\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.4, random_state = 42)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the Titanic dataset was appropriately split\n",
    "for dataset in [y_train, y_validation, y_test]:\n",
    "    print(round(len(dataset) / len(labels), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv('../../Data/TitanicTrainingFeatures.csv', index = False)\n",
    "x_validation.to_csv('../../Data/TitanicValidationFeatures.csv', index = False)\n",
    "x_test.to_csv('../../Data/TitanicTestingFeatures.csv', index = False)\n",
    "\n",
    "y_train.to_csv('../../Data/TitanicTrainingLabels.csv', index = False)\n",
    "y_validation.to_csv('../../Data/TitanicValidationLabels.csv', index = False)\n",
    "y_test.to_csv('../../Data/TitanicTestingLabels.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression: A statistical process for estimating the relationships among variables, \n",
    "# often to make a prediction about some outcome\n",
    "\n",
    "# Logistic Regression: A form of regression where the target variable is binary\n",
    "# General Guidelines:\n",
    "# Use When:\n",
    "# 1. Binary target variable\n",
    "# 2. Transparency is important or interested in significance of predictors\n",
    "# 3. Fairly well behaved data\n",
    "# 4. Need a quick initial benchmark\n",
    "# Avoid When:\n",
    "# 1. Continous target variable\n",
    "# 2. Massive data (rows or columns)\n",
    "# 3. Unwieldy data\n",
    "# 4. Performance is the only thing that matters\n",
    "\n",
    "# It is unnecessary to tweak every possible hyperparameter, only those that are most important or have the \n",
    "# greatest impact upon the model\n",
    "\n",
    "# C Hyperparameter: Regularization parameter in logistic regression that controls how closely the model fits\n",
    "# to the training data\n",
    "# C = 1 / lambda, where lambda is the regularization paramater\n",
    "# lambda -> 0 = C -> Infinity, Low regularization, high complexity, more likely to overfit\n",
    "# lambda -> Infinity = C -> 0, High regularization, low complexity, more likely to underfit\n",
    "\n",
    "# Regularization: Technique used to reduce overfitting by discouraging overly complex models in some way\n",
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods available within LogisticRegression\n",
    "# All sklearn models have the same APIs\n",
    "dir(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier to understand results\n",
    "def print_results(results):\n",
    "    print('Best Parameters: {}\\n'.format(results.best_params_))\n",
    "    \n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    \n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f058490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training_features = x_train\n",
    "#training_labels = y_train\n",
    "\n",
    "#training_features = pd.read_csv('../../Data/TitanicTrainingFeatures.csv')\n",
    "#training_labels = pd.read_csv('../../Data/TitanicTrainingLabels.csv', header = None)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Dictionary to test various parameters with the logistic regression model\n",
    "lr_parameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# GridSearch carrying out k-fold cross validation, the 'cv' value splitting the training data in 5\n",
    "lr_cv = GridSearchCV(lr, lr_parameters, cv = 5)\n",
    "#cv.fit(training_features, training_labels.values.ravel())\n",
    "lr_cv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print_results(lr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Out A Pickled Model\n",
    "joblib.dump(lr_cv.best_estimator_, '../../Models/TitanicLogisticRegressionModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM): \n",
    "# Classifier that finds an optimal hyperplane that maximized the margin between two classes\n",
    "# In 2D, the hyperplane is the line that seperates squares and circles on a flat grid, whilst the \n",
    "# support vector / margin is a perendicular line from the hyperplane which has the maximum possible distance\n",
    "# between the two nearest objects of different classes, the 'longer' this line the better\n",
    "# In 3D these become planes with the same goal\n",
    "\n",
    "# Kernel Trick (Method):\n",
    "# Transforms data that is not linearly seperable in n-dimensional space to a higher dimension where it is \n",
    "# linearly seperable, a straight line or flat hyperplane is the only tool allowed to be used, no circles\n",
    "# non straight lines\n",
    "# If you needed a circle to seperate classes in 2D, in 3D a flat plane could be used\n",
    "\n",
    "# General Guidelines:\n",
    "# Use When:\n",
    "# 1. Best for classification with binary target variable, though can be used on categorical and continous outputs,\n",
    "# not as useful for regression\n",
    "# 2. Feature-to-row ratio is high, so many columns but relatively few rows, distinguishing features of SVM\n",
    "# 3. Very complex relationships\n",
    "# 4. Many outliers\n",
    "\n",
    "# Avoid When:\n",
    "# 1. Feature-to-row ratio is low, many rows but few columns, SVM is slow\n",
    "# 2. Transparency is important or interested in significance of predictors\n",
    "# 3. Looking for a quick benchmark model, SVM takes longer to train and output model results\n",
    "SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C Hyperparamter:\n",
    "# C -> Infinity, large penalty for misclassification of data, small margin, overfitting\n",
    "# C -> 0, low penalty for misclassification of data, large margin, underfitting\n",
    "\n",
    "# Model object\n",
    "svc = SVC()\n",
    "\n",
    "svc_parameters = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10]\n",
    "    \n",
    "}\n",
    "\n",
    "svc_cv = GridSearchCV(svc, svc_parameters, cv = 5)\n",
    "svc_cv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "# 'linear' kernel doing much better than rbf kernel, suggesting the data is linearly seperable\n",
    "# 'C' value does not seem to impact linear kernal utilizing models at all\n",
    "print_results(svc_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72620444",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(svc_cv.best_estimator_, '../../Models/TitanicSupportVectorMachineModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron:\n",
    "# Classic feed-forward artificial neural network, the core component of deep learning\n",
    "# A connected series of nodes in the form of an directed acyclic graph where each node represents \n",
    "# a function or a model \n",
    "\n",
    "# Directed Acyclic Graph: Directionality between the nodes and no node will ever be revisited\n",
    "\n",
    "# Every node is connected to every over node of the layer directly next to it in one direction, neural network\n",
    "# Input Layer: 4 Nodes, Age, Class, Cabin, Sex\n",
    "# Hidden Layer: As many nodes as desired\n",
    "# Output Layer: 2 Nodes, Survived, Perished\n",
    "\n",
    "# General Guidelines:\n",
    "# Use When:\n",
    "# 1. Categorical or continous target variables, great for classifcation and regression\n",
    "# 2. Very complex relationships or performance is the only thing that matters\n",
    "# 3. When control over the training process is very important, many hyperparameters available for tuning\n",
    "\n",
    "# Avoid When:\n",
    "# 1. Image recognition, time series, multi layer perceptrons is not the same as deep learning\n",
    "# 2. Transparency is important or interested in significance of predictors\n",
    "# Like blackboxes, these take in inputs and output predictions, but what happens in between is a mystery\n",
    "# 3. Need a quick benchmark model, lots of hyperparameters to tune, slow to train (faster than SVM)\n",
    "# 4. Limited data available, these models thrive when lots of data is available\n",
    "\n",
    "# Important Hyperparameters\n",
    "# Hidden Layer: Determines how many hidden layers there will be and how many nodes in each layer\n",
    "\n",
    "# Activation Function: Dictates the type of nonlinearity that is introduced in the model\n",
    "# Sigmoid Curve: \n",
    "# Hyperbolic Tangent Curve (TanH):\n",
    "# Rectified Linear Unit (ReLU): Default\n",
    "\n",
    "# Learning Rate: Facilitates how quickly and whether or not the algorithm will find the optimal solution\n",
    "# To large a learning rate and the model will never find the optimal value on the loss curve, faster\n",
    "# To low a learning rate and, since most loss curves have local maxima and minima, it might find a local and not\n",
    "# an absolute minima on the loss curve, missing the optimal value, longer training time\n",
    "# MLP has an initial learning rate built into MLP that signifies the starting point for the learning rate\n",
    "\n",
    "MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4f6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp_parameters = {\n",
    "    # (Nodes per layer, Layers)\n",
    "    'hidden_layer_sizes': [(10, ), (50, ), (100, )],\n",
    "    \n",
    "    \n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    \n",
    "    # How the learning rate changes throughout the optimization process\n",
    "    # constant: Initial learning rate stays the same throughout\n",
    "    # invscaling: Slowly decreases learning rate through each step, starting large and becomming smaller over time\n",
    "    # adaptive: Learning rate is constant as love as the training loss keeps decreasing, otherwise is gets smaller\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp_cv = GridSearchCV(mlp, mlp_parameters, cv = 5)\n",
    "\n",
    "# 27 sperate models being fit\n",
    "mlp_cv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "# Convergence warnings on maximum iterations, which is how many times the MLPClassifier will allow to find\n",
    "# an optimal model before it stops trying to optimize\n",
    "print_results(mlp_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0334cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mlp_cv.best_estimator_, '../../Models/TitanicMultilayerPerceptronModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d664fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest:\n",
    "# Merges a collection of independent decision trees to get a more accurate and stable prediction\n",
    "# Is essentially an ensemble method, each tree within the model is built independently and does not know what\n",
    "# the other trees are doing\n",
    "# Swiss Armt Knife of models, uses a majority votes for prediction\n",
    "\n",
    "# General Guidelines:\n",
    "# Use When:\n",
    "# 1. Categorical or continous target variables, great for classifcation and regression\n",
    "# 2. Interested in signficance of predictors, lays out each of their importance\n",
    "# 3. Need a quick benchmark model, flexible in terms of acceptable data, good results, relatively quick to train\n",
    "# 4. Deals really well with messy data, such as outliers and missing values\n",
    "\n",
    "# Avoid When:\n",
    "# 1. Not the best model if trying to solve a complex, novel problem, 90% vs 100%\n",
    "# 2. Transparency is important, hard to see details in a model with possible 100s of decision trees\n",
    "# 3. Quick to train, not the fastest to make predictions\n",
    "RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffe832",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'n_estimators': How many individual decision trees will be built, width of the model\n",
    "# 'max_depth': How many layer deep a decision tree can keep splitting into, controls the complexity of the model\n",
    "# and how close it can fit to the training data, otherwise it can potentially split down to every individual record\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_parameters = {\n",
    "    'n_estimators': [5, 50, 250],\n",
    "    'max_depth': [2, 4, 8, 16, 32, None]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(rf, rf_parameters, cv = 5)\n",
    "\n",
    "rf_cv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print_results(rf_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a19e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_cv.best_estimator_, '../../Models/TitanicRandomForestModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2648a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting: Gradient Boosted Trees is the current example, many more\n",
    "# Ensemble method that aggregates a number of weak models to create one strong model\n",
    "# A weak model is one that is only slightly better than random guessing\n",
    "# A strong model is one that is strongly correlated with true classification\n",
    "# Boosting effectively learns from its mistakes every iteration\n",
    "# Each model learns from the mistakes of its predecessors, and they are not independent from one another\n",
    "# Each time a small shallow model completes, its performance is evalauted, and those predictions it got wrong\n",
    "# are resampled amd overweighted for the next model to attempt and solve, assuming the first model has a gone grasp\n",
    "# on the points it has already correctly classified\n",
    "\n",
    "# One of the most powerful and used in machine learning, should be considered for just about any problem\n",
    "\n",
    "# Since training models are not independent, they are not parallelizable, but the predictions are however\n",
    "# Slow to train, quick to predict, uses weighted voting for prediction \n",
    "# based on how each model performed in training\n",
    "\n",
    "# General Guidelines:\n",
    "# Use When:\n",
    "# 1. Categorical or continous target variables, great for classifcation and regression\n",
    "# 2. Very flexible, handles many different data type, tends to perform better than Random Forest\n",
    "# 3. Easy to find significance of individual predictors to find insights and relationships within data\n",
    "# 4. Prediction time important, very quick to predict\n",
    "\n",
    "# Avoid When:\n",
    "# 1. Transparency is important, same issues as Random Forest with many different trees\n",
    "# 2. Training time is imporantant and compute power limited\n",
    "# 3. Data is really noisy, as boosting is always trying to fix it's own mistakes and tends to overfit by \n",
    "# trying to fit to noise and outliers\n",
    "\n",
    "GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585df104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'learning_rate': The actual learning rate as a constant value throughout the model creation process, as \n",
    "# opposed Multilayer Perceptrons how the learning rate changes\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb_parameters = {\n",
    "    # Uses more but more shallow decision trees than random forest\n",
    "    # Due to how each model optimizes for the bias variance tradeoff\n",
    "    'n_estimators': [5, 50, 250, 500],\n",
    "    'max_depth': [1, 3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "gb_cv = GridSearchCV(gb, gb_parameters, cv = 5)\n",
    "\n",
    "gb_cv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print_results(gb_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7353e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gb_cv.best_estimator_, '../../Models/TitanicGradientBoostingModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb602c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Considerations:\n",
    "# Accuracy:\n",
    "# Training Time: Logistic Regression\n",
    "# Prediction Time: Gradient Boosting\n",
    "\n",
    "# Latency:\n",
    "# How are different sized data sets handled?\n",
    "# How are complex feature relationships handled?\n",
    "# How will they handle messy data?\n",
    "\n",
    "# Reading in the models\n",
    "models = {}\n",
    "\n",
    "for model in ['LogisticRegression', \n",
    "              'SupportVectorMachine', \n",
    "              'MultilayerPerceptron', \n",
    "              'RandomForest', \n",
    "              'GradientBoosting']:\n",
    "    models[model] = joblib.load('../../Models/Titanic{}Model.pkl'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25578c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89526b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics:\n",
    "# Accuracy = # predicted correctly / total # of examples\n",
    "# Precision = # predicted as surviving that actually survived / total # predicted to survive\n",
    "# Recall = # predicted as surviving that actually survived / total # that actually survived\n",
    "\n",
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    \n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    accuracy = round(accuracy_score(labels, predictions), 3)\n",
    "    precision = round(precision_score(labels, predictions), 3)\n",
    "    recall = round(recall_score(labels, predictions), 3)\n",
    "    \n",
    "    print('{} - Accuracy: {} | Precision: {} | Recall: {} | Latency: {} \\n'.format(name, accuracy, precision, \n",
    "                                                                                   recall, round(end - start, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81838e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Models On Validation Dataset:\n",
    "\n",
    "# No Free Lunch Theorem: No single algorithm works best for every problem\n",
    "\n",
    "# x_validation: Features\n",
    "# y_validation: Labels\n",
    "\n",
    "# Typical tradeoff between precision vs. recall\n",
    "# Model selection based on business use case, like filtering spam (precision) vs. fraud detection (recall)\n",
    "\n",
    "# Will always get the same results on the same data as we are utilizing, stored, fit and concrete models\n",
    "# When training models results can be different, but not in this case\n",
    "for name, model in models.items():\n",
    "    evaluate_model(name, model, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Best Model On Testing Dataset:\n",
    "evaluate_model('RandomForest', models['RandomForest'], x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
